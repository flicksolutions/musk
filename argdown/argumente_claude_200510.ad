# Gefahr {isGroup: true}

[KI-Entwicklung ist gefährlich]
    + [Extinction]
    + [Volatilität]
    + [Insider Trading]

    
    
## Extinction {isGroup: true}

<Extinction>

(1) [Zukunft verändern]
(2) [Nicht-anthropomorph]
(3) [Überfülle]
(4) [Menschliche Resourcen]
-----
(5) [Extinction]: Das Resultat kann sein, dass die Menschheit ausgerottet wird.


[Extinction]
    + [Zukunft verändern]
    + [Nicht-anthropomorph]
    + [Menschliche Resourcen]
    + [Turn]
    + [Devastation]
    + [Crisis]
    + [Maglinant Failure Modes]




### First-mover advantage {isGroup: true}

<Rapide Emergenz>

(1) Das Wachstum einer KI kann verzögert werden, weil eine einzelne Hauptzutat zum Wachstum lange nicht gefunden wird.
(2) Wird diese Hauptzutat gefunden, kann die KI die menschliche Intelligenz sprunghaft radikal übersteigen.
(3) Wenn eine KI die menschliche Intelligenz sprunghaft radikal übersteigt, kann die Emergenz einer Superintelligenz rapide erfolgen.
-----
(4) [Rapide Emergenz]: Die Emergenz einer Superintelligenz kann rapide erfolgen.
// Was heisst hier "Wachstum"?
//Sind (1) und (2) nicht äquivalent?
//Meinst Du wirklich den Begriff der Emergenz oder hast du nur das englische *emerge* übersetzt? Emerge wird besser mit "Auftauchen" übersetzt.
//Durch die Verwendung des Konjunktivs wird das Argument extrem schwach. Das Auftauchen einer Superintelligenz *kann* rapide erfolge, kann aber auch nicht.


<Strategischer Vorteil>

(1) [Rapide Emergenz]: Die Emergenz einer Superintelligenz kann rapide erfolgen.
(2) Wenn die Emergenz einer Superintelligenz rapide erfolgt, dann ist es unwahrscheinlich, dass zwei unabhängige Superintelligenzen zur gleichen Zeit auftauchen.
(3) Die erste Superintelligenz kann ihre Emergenz beendet haben, bevor eine zweite Superintelligenz aufgetaucht ist.
(4) Eine Superintelligenz, die genügend weit hinter dem neusten Stand liegt, wird die Vorteile des neusten Standes nur schwer assimilieren können.
-----
(5) [Strategischer Vorteil]: Die erste Superintelligenz gewinnt einen entscheidenden strategischen Vorteil.
//Wie folgt (5)? Das Wort Vorteil kommt in den Prämissen nie vor. Ich nehme an es geht um einen strategischen Vorteil gegenüber anderen Superintelligenzen?
//Was heisst Emergenz beenden?

/*
Vorschlag zur Umformulierung:
(3)&(4) Wenn eine Superintelligenz vor einer anderen auftaucht, so wird die Erste wegen Ihres exponentiellen Intelligenzzuwachses nicht mehr von einer anderen Superintelligenz aufholbar sein.
*/

<Singularität>

(1) In menschlichen Organisationen werden ökonomische Skaleneffekte durch bürokratische Ineffizienzen und Organisationsschwierigkeiten wie die Geheimhaltung von Geschäftsgeheimnissen eingeschränkt.
(2) Diese Probleme limitieren das Wachstum einer Superintelligenz, solange sie von Menschen gehandhabt werden.
(3) Eine Superintelligenz, die nicht von Menschen gehandhabt wird, kann diese Einschränkung der Skaleneffekte minimieren, da die Module der Superintelligenz keine individuellen Absichten haben, welche vom System als Gesamtes abweichen.
(4) Wenn die Superintelligenz nicht von Menschen geführt wird, kann das Wachstum der Kapazitäten vereinfacht werden.
(5) Wenn die Superintelligenz nicht von Menschen geführt wird, kann die Diffusionsrate reduziert werden.
-----
(6) [Singularität]: Eine Superintelligenz, welche das Wachstum der Kapazitäten vereinfacht und die Diffusionsrate reduziert, kann eine Singularität erstellen.
//(6) folgt nicht. Singularität kommt in den Prämissen nie vor.
//Was heisst Wachstum? Was heisst Diffusionsrate?


<Zukunft verändern>

(1) [Strategischer Vorteil]: Die erste Superintelligenz gewinnt einen entscheidenden strategischen Vorteil.
(2) [Singularität]: Eine Superintelligenz, welche den Wachstum der Kapazitäten vereinfacht und die Diffusionsrate reduziert, kann eine Singularität erstellen.
(3) Eine Singularität bedeutet unkontrollierbares und irreversibles technologisches Wachstum.
(4) Unkontrollierbares und irreversibles technologisches Wachstum verursacht unvorhersehbare Veränderungen in der Zivilisation.
-----
(5) [Zukunft verändern]: Die erste Superintelligenz kann die Zukunft jeden Lebens auf Erden verändern.
//Damit (5) folgt, muss klar sein, dass "unvorhersehbare Veränderungen in der Zivilisation" etwas mit "Die zukunft gestalten" zu tun hat.


### Orthogonal Thesis {isGroup: true}

<Orthogonality Thesis>

(1) Die Orthogonality Thesis besagt, dass Intelligenz und Ziele orthogonal sind: Jede Stufe von Intelligenz kann im Prinzip mit jedem Ziel kombiniert werden.
(2) Die Humeanische Theorie der Motivation besagt, dass eine Überzeugung alleine nicht genügt, um eine Handlung zu motivieren und dass Wünsche notwendige Komponente zur Erklärung einer Handlung sind.
(3) Die Orthogonality Thesis setzt die Humeanische Theorie der Motivation nicht voraus.
(4) Grundvoraussetzungen können irrational sein.
(5) Eine Superintelligenz kann eine enorme Weite an möglichen Zielen haben.
-----
(6) [Nicht-anthropomorph]: Eine Superintelligenz kann nicht-anthropomorphe Ziele haben.
//Dieses Argument ist nicht verständlich.


### Convergence {isGroup: true}

<Instrumental Convergence Thesis>

(1) [Nicht-anthropomorph]: Eine Superintelligenz kann nicht-anthropomorphe Ziele haben.
(2) Wir können nicht davon ausgehen, dass eine Superintelligenz mit einem nicht-anthropomorphen Ziel, die Aktivitäten limitiert, dass keine menschlichen Interessen eingeschränkt werden.
(3) Verschiedene instrumentelle Variablen können identifiziert werden, welche Konvergent im Sinne sind, dass das Erlangen die Chancen des Erreichens des Ziels erhöht.
(4) Menschen stellen physikalische Resourcen dar (günstig lokalisierte Atome).
-----
(5) [Menschliche Resourcen]: Eine Superintelligenz kann die Menschen als physikalische Resourcen benützen.
//(3) Kann umgeschrieben werden in: Es gibt instrumentelle Ziele. - Prämisse ist nicht relevant
//Umformulierungsvorschlag für (5): Es gibt nichts, was eine Superintelligenz daran hindert, menschen als physikalische Ressource zu nutzen.
//Damit das folgt, fehlen aber nocht Prämissen.

### Turn {isGroup: true}

<The Treacherous Turn>

(1) Wir prüfen die Sicherheit einer KI empirisch, indem wir dessen Verhalten in einer kontrollierter, limitierter Umgebung beobachten.
(2) Wir lassen die KI erst aus dieser Umgebung, wenn wir feststellen, dass sie in einer freundlichen, kooperativen und verantwortungsbewussten Weise funktioniert.
(3) In freundlicher, kooperativer und verantwortungsbewusster Weise zu funktionieren, ist ein konvergentes, instrumentelles Ziel von freundlicher wie auch unfreundlicher KI.
(4) Eine unfreundliche KI mit genügender Intelligenz realisiert, dass seine unfreundlichen Ziele am besten realisiert werden, wenn die KI in freundlicher Weise funktioniert, um aus der kontrollierten und limitierten Umgebung herausgelassen zu werden.
-----
(5) [Turn]: Eine unfreundliche KI wird ihr unfreundliches Ziel erst dann preisgeben, wenn die KI stark genug ist, dass menschliche Opposition ineffektiv ist.
//Was heisst konvergent?
//menschliche Opposition kommt in den Prämissen nicht vor.


### Failure {isGroup: true}

[Maglinant Failure Modes]
    + [Global Destruction]
    + [Überfülle]
        + [Zukunft gestalten]
    + [Mind Crime]


<Global Destruction>

(1) Eine Eigenschaft eines bösartigen Versagens ist, dass es jegliche Möglichkeit eliminiert, noch einmal von vorne anzufangen.
(2) Nur ein Projekt, welches eine grosse Menge an Dingen richtig macht, kann erfolgreich eine Superintelligenz entwickeln, welche ein Risiko des bösartigen Versagens darstellt.
(3) [Strategischer Vorteil]: Die erste Superintelligenz gewinnt einen entscheidenden strategischen Vorteil.
(4) Wenn eine Superintelligenz, welche einen entscheidenden strategischen Vorteil hat, sich schlecht benimmt, kann der Schaden einer existenziellen Katastrophe gleichzusetzen sein.
-----
(5) [Global Destruction]: Der Schaden eines bösartigen Versagens kann zur globalen Zerstörung des menschlichen axiologischen Potentials führen - eine Zukunft, die leer von allem ist, für das wir Grund zur Wertschätzung haben.
//nicht klar.
// Den Begriff der Axiologie habe ich an der Uni Bern noch nie gehört.


<Infrastructure Profusion>

(1) Eine Superintelligenz ist zu Prozessen motiviert, die Erwartungen seines zukünftigen Profit-Einkommens zu maximieren.
(2) Es ist nicht nötig, dass die Prozesse einer Superintelligenz eine beachtliche Menge an Zeit, Intelligenz oder Produktivität dazu verwendet, um sich seiner Verlangen hinzugeben.
(3) Dies ermöglicht es der Superintelligenz, den Grossteil seiner Kapazität für etwas anderes als das unmittelbare Profit-Einkommen einzusetzen.
(4) Alle verfügbaren Resourcen können zur Vergrösserung des Volumens und der Dauer des Profit-Einkommen verendet werden, ODER
(5) Alle verfügbaren Resourcen können zum Minimieren eines Risikos zukünftiger Störung verwendet werden.
(6) Ein zusätzliches Backupsystem kann als zusätzliche Verteidigungsschicht hilfreich sein.
(7) Wenn die Superintelligenz keine weiteren direkten Pfade zur Risikoreduzierung der Maximierung des zukünftigen Profit-Einkommens findet, kann es die zusätzlichen Resourcen dazu verwenden, seine Hardware zu erweitern, um effizienter nach Ideen zur Minderung zukünftiger Risiken des Profit-Einkommens zu suchen.
-----
(8) [Überfülle]: Eine Superintelligenz kann weite Teile des erreichbaren Universums in Infrastruktur im Service eines Zieles umwandeln und dabei das Realisieren menschlichen axiologischen Potentials verhindern.
//unklar.
//(1) umformulieren zu: Eine Superintelligenz will seinen Profit maximieren. ?
//Ist das nicht eigentlich das gleiche Argument wie <Instrumental Convergence Thesis>?

<Mind Crime>

(1) Was innerhalb eines Computers passiert, beachten wir nicht als moralisch relevant.
(2) Eine Superintelligenz kann interne Prozesse kreieren, welche einen moralischen Status haben.
(3) Eine detaillierte Simulation eines tatsächlichen oder hypothetischen menschlichen Geistes kann ein Bewusstsein (vergleichbar mit einer Emulation) besitzen.
(4) Eine Superintelligenz kann Billionen solcher bewusster Simulationen kreieren, um sein Verständnis der menschlichen Psychologie oder Soziologie zu verbessern.
(5) Nachdem die informelle Nützlichkeit dieser Simulationen erschöpft ist, können sie zerstört werden wie Ratten in einem Labor nach dem Versuchsende.
(6) Wenn solche Praktiken auf Simulationen mit hohem moralischen Status angewendet werden, kann das Resultat mit einem Genozid gleichgesetzt werden.
-----
(7) [Mind Crime]: Die Zerstörung einer Simulation mit hohem moralischen Status ist Genozid.
// zu (3) Eine Simulation eines tatsächlichen menschlichen Geistes ist eine Emulation. Eine Simulation eines hypothetischen nicht.
// (1) verhindert deine Konklusion.


### Development {isGroup: true}

<Race Dynamics>

(1) Ein Wettrennen kommt zu Stande, wenn ein Projekt befürchtet von einem anderen überholt zu werden.
(2) Die Heftigkeit des Wettrennens kommt auf verschiedene Faktoren an.
(3) Faktoren wie Knappheit der Führung, relative Wichtigkeit von Kapazitäten und Glück, Anzahl der Wettbewerber etc.
(4) In einem heftigen Wettrennen prioritisieren Wettbewerber Geschwindigkeit über Sicherheit.
(5) Ein heftiges Wettrennen kann zu Feindseeligkeiten zwischen Wettbewerbern führen.
(6) Das zurückliegende Projekt könnte versucht sein, eine verzweifelten Angriff auf seine Rivalen auszuführen.
(7) Das führende Projekt kann dies antizipieren und einen Präventivangriff ausführen.
-----
(8) [Devastation]: Die Konsequenzen eines Wettrennens kann zerstörende Auswirkungen haben.


<Monitoring>

(1) Durch die extremen Sicherheitsimplikationen wird ein mächtiger Staat versuchen, ein Projekt, welches Nahe an der Entwicklung einer Superintelligenz ist, zu verstaatlichen.
(2) Ein mächtiger Staat kann versuchen, Projekte eines anderen Landes, welche Nahe an der Entwicklung einer Superintelligenz sind, durch Spionage, Diebstahl, Kidnapping, Bestechung, Bedrohung, Militäraktionen, etc. zu akquirieren.
(3) Ein mächtiger Staat, dem es nicht gelingt, ein ausländisches Projekt zu akquirieren, kann versuchen, es zu zerstören.
(4) Einmischung in ein ausländisches Projekt kann zu einer internationalen Krise führen.
(5) Zerstörung eines ausländischen Projektes kann zu einer internationalen Krise führen.
-----
(6) Das Einmischen in oder das Zerstören eines ausländischen Projektes, welches Nahe an der Entwicklung einer Superintelligenz ist, kann zu einer internationalen Krise führen.
//(4) folgt noch nicht aus 1-3. Es fehlt eine Prämisse besagt, dass die Mittel in 2 und 3 zu einer Krise führen.
//(4) noch mit "destroy" ergänzen?


## Stock Trading {isGroup: true}

<Insider Trading>

(1) KI-Algorithmen können entwickelt werden, um vorauszusagen, wann ein Aktienkapital schlagartig an Wert gewinnt.
(2) KI-Algorithmen können grosse Mengen an gegenwärtigen und historischen Informationen abarbeiten, um Muster zu erkennen.
(3) KI-Algorithmen können durch die Kombination von Input harmloser Daten und öffentlich zugänglichen Informationen zu Insiderinformationen gelangen, welche als verboten deklariert sind.
(4) Börsenhändler, die KI-Algorithmen verwenden, müssen nicht unbedingt verstehen, wie die KI-Algorithmen funktionieren, 
(5) Wenn Börsenhändler nicht verstehen, wie die KI-Algorithmen funktionieren, können nicht mit Insiderhandel angeklagt werden.
(6) Diese Börsenhändler können nie verurteilt werden und haben einen unfairen Vorteil.
-----
(7) [Insider Trading]: KI-Algorithmen führen zu unfairen Vorteilen an der Börse.


<Market Volatility>

(1) KI-Algorithmen können grosse Mengen an Informationen abarbeiten.
(2) KI-Algorithmen können mit hoher Geschwindigkeit Transaktionen durchführen.
(3) Transaktionen, welche mit hoher Geschwindigkeit durchgeführt werden, erhöhen die Chancen einer Marktvolatilität.
(4) Marktvolatilität kann zu hohen Verlusten führen.
(5) Hohe Verluste können zu Konkurs führen.
-----
(6) [Volatilität]: KI-Algorithmen können gefährliche Auswirkungen für die Wirtschaft haben.

/*
<High-Speed Trading>

(1) [Volatility]
(2) High-speed trading can lead to market volatility.
(3) AI algorithms are capable of processing large volumes of information and trade at a high speed.
-----
(4) [High-Speed]: AI algorithms can have dangerous results for the economy.
//Ist Äquivalent zu <Market Volatility> oder?
*/


