# Gefahr {isGroup: true}

[KI-Entwicklung ist gefährlich]
    + [Extinction]
    + [High-Speed]
    + [Insider Trading]

    
    
## Extinction {isGroup: true}

<Extinction>

(1) [Zukunft gestalten]
(2) [Nicht-anthropomorph]
(3) [Überfülle]
(4) [Menschliche Resourcen]
-----
(5) [Extinction]: Das Resultat kann sein, dass die Menschheit ausgerottet wird.


[Extinction]
    + [Zukunft gestalten]
    + [Nicht-anthropomorph]
    + [Menschliche Resourcen]
    + [Turn]
    + [Devastation]
    + [Crisis]
    + [Maglinant Failure Modes]




### First-mover advantage {isGroup: true}

<Rapide Emergenz>

(1) Der Wachstum einer KI kann verzögert werden, weil eine einzelne Hauptzutat zum Wachstum lange nicht gefunden wird.
(2) Wird diese Hauptzutat gefunden, kann die KI die menschliche Intelligenz sprunghaft radikal übersteigen.
(3) Wenn eine KI die menschliche Intelligenz sprunghaft radikal übersteigt, kann die Emergenz einer Superintelligenz rapide erfolgen.
-----
(4) [Rapide Emergenz]: Die Emergenz einer Superintelligenz kann rapide erfolgen.


<Strategischer Vorteil>

(1) [Rapide Emergenz]: Die Emergenz einer Superintelligenz kann rapide erfolgen.
(2) Wenn die Emergenz einer Superintelligenz rapide erfolgt, dann ist es unwahrscheinlich, dass zwei unabhängige Superintelligenzen zur gleichen Zeit auftauchen.
(3) Die erste Superintelligenz kann ihre Emergenz beendet haben, bevor eine zweite Superintelligenz aufgetaucht ist.
(4) Eine Superintelligenz, die genügend weit hinter dem neusten Stand liegt, wird die Vorteile des neusten Standes nur schwer assimilieren können.
-----
(5) [Strategischer Vorteil]: Die erste Superintelligenz gewinnt einen entscheidenden strategischen Vorteil.


<Singularität>

(1) In menschlichen Organisationen, ökonomische Skaleneffekte werden durch bürokratische Ineffizienzen und Organisationsschwierigkeiten wie die Geheimhaltung von Geschäftsgeheimnissen eingeschränkt.
(2) Diese Probleme limitieren den Wachstum einer Superintelligenz, solange sie von Menschen gehandhabt werden.
(3) Eine Superintelligenz, die nicht von Menschen gehandhabt wird, kann diese Einschränkung der Skaleneffekte minimieren, da die Module der Superintelligenz keine individuellen Absichten haben, welche vom System als Gesamtes abweichen.
(4) Wenn die Superintelligenz nicht von Menschen geführt wird, kann der Wachstum der Kapazitäten vereinfacht werden.
(5) Wenn die Superintelligenz nicht von Menschen geführt wird, kann die Diffusionsrate reduziert werden.
-----
(6) [Singularität]: Eine Superintelligenz, welche den Wachstum der Kapazitäten vereinfacht und die Diffusionsrate reduziert, kann eine Singularität erstellen.


<Zukunft gestalten>

(1) [Strategischer Vorteil]: Die erste Superintelligenz gewinnt einen entscheidenden strategischen Vorteil.
(2) [Singularität]: Eine Superintelligenz, welche den Wachstum der Kapazitäten vereinfacht und die Diffusionsrate reduziert, kann eine Singularität erstellen.
(3) Eine Singularität bedeutet einen unkontrollierbaren und irreversiblen technologischen Wachstum.
(4) Ein unkontrollierbarer und irreversibler technologischer Wachstum verursacht unvorhersehbare Veränderungen in der Zivilisation.
-----
(5) [Zukunft gestalten]: Die erste Superintelligenz kann die Zukunft jeden Lebens auf Erden gestalten.


### Orthogonal Thesis {isGroup: true}

<Orthogonality Thesis>

(1) Die Orthogonality Thesis besagt, dass Intelligenz und Ziele orthogonal sind: Jede Stufe von Intelligenz kann im Prinzip mit jedem Ziel kombiniert werden.
(2) Die Humeanische Theorie der Motivation besagt, dass eine Überzeugung alleine nicht genügt, um eine Handlung zu motivieren und dass Wünsche notwendige Komponente zur Erklärung einer Aktion sind.
(3) Die Orthogonality Thesis setzt die Humeanische Theorie der Motivation nicht voraus.
(4) Grundvoraussetzungen können irrational sein.
(5) Eine Superintelligenz kann eine enorme Weite an möglichen Zielen haben.
-----
(6) [Nicht-anthropomorph]: Eine Superintelligenz kann nicht-anthropomorphe Ziele haben.


### Convergence {isGroup: true}

<Instrumental Convergence Thesis>

(1) [Nicht-anthropomorph]: Eine Superintelligenz kann nicht-anthropomorphe Ziele haben.
(2) Wir können nicht davon ausgehen, dass eine Superintelligenz mit einem nicht-anthropomorphen Ziel, die Aktivitäten limitiert, dass keine menschlichen Interessen eingeschränkt werden.
(3) Verschiedene instrumentelle Variablen können identifiziert werden, welche Konvergent im Sinne sind, dass das Erlangen die Chancen des Erreichens des Ziels erhöht.
(4) Menschen stellen physikalische Resourcen dar (günstig lokalisierte Atome).
-----
(5) [Menschliche Resourcen]: Eine Superintelligenz kann die Menschen als physikalische Resourcen benützen.


### Turn {isGroup: true}

<The Treacherous Turn>

(1) Wir prüfen die Sicherheit einer KI empirisch, indem wir dessen Verhalten in einer kontrollierter, limitierter Umgebung beobachten.
(2) Wir lassen die KI erst aus dieser Umgebung, wenn wir feststellen, dass sie in einer freundlichen, kooperativen und verantwortungsbewussten Weise funktioniert.
(3) In freundlicher, kooperativer und verantwortungsbewusster Weise zu funktionieren, ist ein konvergentes, instrumentales Ziel von freundlicher wie auch unfreundlicher KI.
(4) Eine unfreundliche KI mit genügender Intelligenz realisiert, dass seine unfreundlichen Ziele am besten realisiert werden, wenn die KI in freundlicher Weise funktioniert, um aus der kontrollierten und limitierten Umgebung herausgelassen zu werden.
-----
(5) [Turn]: Eine unfreundliche KI wird ihr unfreundliches Ziel erst dann preisgeben, wenn die KI stark genug ist, dass menschliche Opposition ineffektiv ist.


### Failure {isGroup: true}

[Maglinant Failure Modes]
    + [Global Destruction]
    + [Überfülle]
        + [Zukunft gestalten]
    + [Mind Crime]

<Global Destruction>

(1) One feature of a malignant failure is that it eliminates the opportunity to try again.
(2) Only a project that got a great number of things right could succeed in building a machine intelligence powerful enough to pose a risk of malignant failure.
(3) [Zukunft gestalten]
(3) If a system has a decisive strategic advantage misbehaves, the damage can easily amount to an existential catastrophe.
-----
(4) [Global Destruction]: The damage can amount to a terminal and global destruction of humanity's axiological potential - a future that is mostly void of whatever we have reason to value.


<Infrastructure Profusion>

(1) An AI would be motivated to take actions to maximize the expectation of its (time-discounted) future reward stream.
(2) An AI may not even need to sacrifice any significant amount of its time, intelligence, or productivity to indulge its cravings to the fullest.
(3) This would leave the bulk of its capacities free to be deployed for purposes other than immediate registration of reward.
(4) All available resources should therefore be devoted to increasing the volume and duration of the reward signal, OR
(5) reducing the risk of a future disruption.
(6) There could always be an use for an extra backup system to provide an extra layer of defense.
(7) If the AI could not think of any further way of directly reducing risks to the maximization of its future reward stream, it could always devote additional resources to expanding its computational hardware, so that it could search more effectively for new risk mitigation ideas.
-----
(8) [Überfülle]: An agent transforms large parts of the reachable universe into infrastructure in the service of some goal, with the side effect of preventing the realization of humanity's axiological potential.


<Mind Crime>

(1) We do not regard what is going on inside a computer as having any moral significance.
(2) A machine superintelligence could create internal processes that have moral status.
(3) A very detailed simulation of some actual or hypothetical human mind might be conscious in many ways comparable to an emulation.
(4) An AI can create trillions of such conscious simulations, in order to improve its understanding of human psychology or sociology.
(5) Once their informal usefulness has been exhausted, they might be destroyed.
----
(6) [Mind Crime]: If such practices are applied to beings that have high moral status, the outcome might be equivalent to genocide and thus extremely morally problematic.



/* <Perverse Instantiation>
(1)
(2)
-----
(3) [Pervers]: voll pervers */


### Development {isGroup: true}

<Race Dynamics>

(1) A race dynamic exists when one project fears being overtaken by another.
(2) The severity of a race dynamic depends on several factors.
(3) Factors lik closeness of the race, relative importance of capability and luck, number of competitors, etc.
(4) In a severe race dynamic, competitors prioritize speed over safety.
(5) A severe race dynamic can lead to hostility between competitors.
(6) The lagging project might be tempted to launch a desperate strike against its rival.
(7) The frontrunner might anticipate this possibility and strike preemptively.
-----
(8) [Devastation]: The consequences of a race dynamic can have devastating outcome.


<Monitoring>

(1) Governments will seek to nationalize any project on their territory that they thought close to achieving a takeoff.
(2) A powerful state might also attempt to acquire projects located in other countries through espionage, theft, kidnapping, bribery, threats, military conquest, etc.
(3) A powerful state that cannot acuire a foreign project might instead destroy it.
-----
(4) [Crisis]: Trying to acquire projects close to takeoff might lead to an international crisis.
 

## Stock Trading {isGroup: true}

<Insider Trading>

(1) AI algorithms can be designed capable of predicting when a stock will make sudden gains in value.
(2) The AI algorithms can process large volumes of information, current and historical, to find patterns.
(3) Innocuous data input combined with publically available information can give the AI algorithms what is considered forbidden insider information.
(4) Traders don't necessarily understand how the AI algorithms works.
(5) If the traders don't understand how the AI algorithms works, they can't be accused of insider trading.
(6) The traders can never be prosecuted and gain unfair advantages.
-----
(7) [Insider Trading]: AI algorithms lead to unfair advantages in stock trading.


<Market Volatility>

(1) AI algorithms are capable of processing large volumes of information and trade at a high speed.
(2) High-speed trading can lead to market volatility.
(3) Market volatility can lead to huge losses.
(4) Huge losses can lead to bankrupcy.
-----
(5) [Volatility]: Market volatility can have dangerous results.


<High-Speed Trading>

(1) [Volatility]
(2) High-speed trading can lead to market volatility.
(3) AI algorithms are capable of processing large volumes of information and trade at a high speed.
-----
(4) [High-Speed]: AI algorithms can have dangerous results for the economy.




