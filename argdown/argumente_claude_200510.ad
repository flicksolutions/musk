# Gefahr {isGroup: true}

[KI-Entwicklung ist gefährlich]
    + [Extinction]
    + [High-Speed]
    + [Insider Trading]

    
    
## Extinction {isGroup: true}

<Extinction>

(1) [Zukunft gestalten]
(2) [Nicht-anthropomorph]
(3) [Überfülle]
(4) [Menschliche Resourcen]
-----
(5) [Extinction]: Das Resultat kann sein, dass die Menschheit ausgerottet wird.


[Extinction]
    + [Zukunft gestalten]
    + [Nicht-anthropomorph]
    + [Menschliche Resourcen]
    + [Turn]
    + [Devastation]
    + [Crisis]
    + [Maglinant Failure Modes]




### First-mover advantage {isGroup: true}

<Rapide Emergenz>

(1) Der Wachstum einer KI kann verzögert werden, weil eine einzelne Hauptzutat zum Wachstum lange nicht gefunden wird.
(2) Wird diese Hauptzutat gefunden, kann die KI die menschliche Intelligenz sprunghaft radikal übersteigen.
(3) Wenn eine KI die menschliche Intelligenz sprunghaft radikal übersteigt, kann die Emergenz einer Superintelligenz rapide erfolgen.
-----
(4) [Rapide Emergenz]: Die Emergenz einer Superintelligenz kann rapide erfolgen.


<Strategischer Vorteil>

(1) [Rapide Emergenz]: Die Emergenz einer Superintelligenz kann rapide erfolgen.
(2) Wenn die Emergenz einer Superintelligenz rapide erfolgt, dann ist es unwahrscheinlich, dass zwei unabhängige Superintelligenzen zur gleichen Zeit auftauchen.
(3) Die erste Superintelligenz kann ihre Emergenz beendet haben, bevor eine zweite Superintelligenz aufgetaucht ist.
(4) Eine Superintelligenz, die genügend weit hinter dem neusten Stand liegt, wird die Vorteile des neusten Standes nur schwer assimilieren können.
-----
(5) [Strategischer Vorteil]: Die erste Superintelligenz gewinnt einen entscheidenden strategischen Vorteil.


<Singularität>

(1) In menschlichen Organisationen, ökonomische Skaleneffekte werden durch bürokratische Ineffizienzen und Organisationsschwierigkeiten wie die Geheimhaltung von Geschäftsgeheimnissen eingeschränkt.
(2) Diese Probleme limitieren den Wachstum einer Superintelligenz, solange sie von Menschen gehandhabt werden.
(3) Eine Superintelligenz, die nicht von Menschen gehandhabt wird, kann diese Einschränkung der Skaleneffekte minimieren, da die Module der Superintelligenz keine individuellen Absichten haben, welche vom System als Gesamtes abweichen.
(4) Wenn die Superintelligenz nicht von Menschen geführt wird, kann der Wachstum der Kapazitäten vereinfacht werden.
(5) Wenn die Superintelligenz nicht von Menschen geführt wird, kann die Diffusionsrate reduziert werden.
-----
(6) [Singularität]: Eine Superintelligenz, welche den Wachstum der Kapazitäten vereinfacht und die Diffusionsrate reduziert, kann eine Singularität erstellen.


<Zukunft gestalten>

(1) [Strategischer Vorteil]: Die erste Superintelligenz gewinnt einen entscheidenden strategischen Vorteil.
(2) [Singularität]: Eine Superintelligenz, welche den Wachstum der Kapazitäten vereinfacht und die Diffusionsrate reduziert, kann eine Singularität erstellen.
(3) Eine Singularität bedeutet einen unkontrollierbaren und irreversiblen technologischen Wachstum.
(4) Ein unkontrollierbarer und irreversibler technologischer Wachstum verursacht unvorhersehbare Veränderungen in der Zivilisation.
-----
(5) [Zukunft gestalten]: Die erste Superintelligenz kann die Zukunft jeden Lebens auf Erden gestalten.


### Orthogonal Thesis {isGroup: true}

<Orthogonality Thesis>

(1) Die Orthogonality Thesis besagt, dass Intelligenz und Ziele orthogonal sind: Jede Stufe von Intelligenz kann im Prinzip mit jedem Ziel kombiniert werden.
(2) Die Humeanische Theorie der Motivation besagt, dass eine Überzeugung alleine nicht genügt, um eine Handlung zu motivieren und dass Wünsche notwendige Komponente zur Erklärung einer Aktion sind.
(3) Die Orthogonality Thesis setzt die Humeanische Theorie der Motivation nicht voraus.
(4) Grundvoraussetzungen können irrational sein.
(5) Eine Superintelligenz kann eine enorme Weite an möglichen Zielen haben.
-----
(6) [Nicht-anthropomorph]: Eine Superintelligenz kann nicht-anthropomorphe Ziele haben.


### Convergence {isGroup: true}

<Instrumental Convergence Thesis>

(1) [Nicht-anthropomorph]: Eine Superintelligenz kann nicht-anthropomorphe Ziele haben.
(2) Wir können nicht davon ausgehen, dass eine Superintelligenz mit einem nicht-anthropomorphen Ziel, die Aktivitäten limitiert, dass keine menschlichen Interessen eingeschränkt werden.
(3) Verschiedene instrumentelle Variablen können identifiziert werden, welche Konvergent im Sinne sind, dass das Erlangen die Chancen des Erreichens des Ziels erhöht.
(4) Menschen stellen physikalische Resourcen dar (günstig lokalisierte Atome).
-----
(5) [Menschliche Resourcen]: Eine Superintelligenz kann die Menschen als physikalische Resourcen benützen.


### Turn {isGroup: true}

<The Treacherous Turn>

(1) Wir prüfen die Sicherheit einer KI empirisch, indem wir dessen Verhalten in einer kontrollierter, limitierter Umgebung beobachten.
(2) Wir lassen die KI erst aus dieser Umgebung, wenn wir feststellen, dass sie in einer freundlichen, kooperativen und verantwortungsbewussten Weise funktioniert.
(3) In freundlicher, kooperativer und verantwortungsbewusster Weise zu funktionieren, ist ein konvergentes, instrumentales Ziel von freundlicher wie auch unfreundlicher KI.
(4) Eine unfreundliche KI mit genügender Intelligenz realisiert, dass seine unfreundlichen Ziele am besten realisiert werden, wenn die KI in freundlicher Weise funktioniert, um aus der kontrollierten und limitierten Umgebung herausgelassen zu werden.
-----
(5) [Turn]: Eine unfreundliche KI wird ihr unfreundliches Ziel erst dann preisgeben, wenn die KI stark genug ist, dass menschliche Opposition ineffektiv ist.


### Failure {isGroup: true}

[Maglinant Failure Modes]
    + [Global Destruction]
    + [Überfülle]
        + [Zukunft gestalten]
    + [Mind Crime]


<Global Destruction>

(1) Eine Eigenschaft eines bösartigen Versagens ist, dass es jegliche Möglichkeit eliminiert, noch einmal von vorne anzufangen.
(2) Nur ein Projekt, welches eine grosse Menge an Dingen richtig macht, kann erfolgreich eine Superintelligenz entwickeln, welche ein Risiko des bösartigen Versagens darstellt.
(3) [Strategischer Vorteil]: Die erste Superintelligenz gewinnt einen entscheidenden strategischen Vorteil.
(4) Wenn eine Superintelligenz, welche einen entscheidenden strategischen Vorteil hat, sich schlecht benimmt, kann der Schaden einer existenziellen Katastrophe gleichzusetzen sein.
-----
(5) [Global Destruction]: Der Schaden eines bösartigen Versagens kann zur globalen Zerstörung des menschlichen axiologischen Potentials führen - eine Zukunft, die leer von allem ist, für das wir Grund zur Wertschätzung haben.


<Infrastructure Profusion>

(1) Eine Superintelligenz ist zu Prozessen motiviert, die Erwartungen seines zukünftigen Profit-Einkommens zu maximieren.
(2) Es ist nicht nötig, dass die Prozesse einer Superintelligenz eine beachtliche Menge an Zeit, Intelligenz oder Produktivität dazu verwendet, um sich seiner Verlangen hinzugeben.
(3) Dies ermöglicht es der Superintelligenz, den Grossteil seiner Kapazität für etwas anderes als das unmittelbare Profit-Einkommen einzusetzen.
(4) Alle verfügbaren Resourcen können zur Vergrösserung des Volumens und der Dauer des Profit-Einkommen verendet werden, ODER
(5) Alle verfügbaren Resourcen können zum Minimieren eines Risikos zukünftiger Störung verwendet werden.
(6) Ein zusätzliches Backupsystem kann als zusätzliche Verteidigungsschicht hilfreich sein.
(7) Wenn die Superintelligenz keine weiteren direkten Pfade zur Risikoreduzierung der Maximierung des zukünftigen Profit-Einkommens findet, kann es die zusätzlichen Resourcen dazu verwenden, seine Hardware zu erweitern, um effizienter nach Ideen zur Minderung zukünftiger Risiken des Profit-Einkommens zu suchen.
-----
(8) [Überfülle]: Eine Superintelligenz kann weite Teile des erreichbaren Universums in Infrastruktur im Service eines Zieles umwandeln und dabei das Realisieren menschlichen axiologischen Potentials verhindern.


<Mind Crime>

(1) Was innerhalb einaes Computers passiert, beachten wir nicht als moralische Wichtigkeit.
(2) Eine Superintelligenz kann interne Prozesse kreieren, welche einen moralischen Status haben.
(3) Eine detaillierte Simulation eines aktualen oder hypothetischen menschlichen Geist kann ein Bewusstsein vergleichbar mit einer Emulation besitzen.
(4) Eine Superintelligenz kann Billionen solcher bewusster Simulationen kreieren, um sein Verständnis der menschlichen Psychologie oder Soziologie zu verbessern.
(5) Nachdem die informelle Nützlichkeit dieser Simulationen erschöpft ist, können sie zerstört werden wie Ratten in einem Labor nach dem Versuchsende.
(6) Wenn solche Praktiken auf Simulationen mit hohem moralischen Status angewendet werden, kann das Resultat mit einem Genozid gleichgesetzt werden.
-----
(7) [Mind Crime]: Die Zerstörung einer Simulation mit hohem moralischen Status ist Genozid.

/* <Perverse Instantiation>
(1)
(2)
-----
(3) [Pervers]: voll pervers */


### Development {isGroup: true}

<Race Dynamics>

(1) A race dynamic exists when one project fears being overtaken by another.
(2) The severity of a race dynamic depends on several factors.
(3) Factors lik closeness of the race, relative importance of capability and luck, number of competitors, etc.
(4) In a severe race dynamic, competitors prioritize speed over safety.
(5) A severe race dynamic can lead to hostility between competitors.
(6) The lagging project might be tempted to launch a desperate strike against its rival.
(7) The frontrunner might anticipate this possibility and strike preemptively.
-----
(8) [Devastation]: The consequences of a race dynamic can have devastating outcome.


<Monitoring>

(1) Governments will seek to nationalize any project on their territory that they thought close to achieving a takeoff.
(2) A powerful state might also attempt to acquire projects located in other countries through espionage, theft, kidnapping, bribery, threats, military conquest, etc.
(3) A powerful state that cannot acuire a foreign project might instead destroy it.
-----
(4) [Crisis]: Trying to acquire projects close to takeoff might lead to an international crisis.
 

## Stock Trading {isGroup: true}

<Insider Trading>

(1) AI algorithms can be designed capable of predicting when a stock will make sudden gains in value.
(2) The AI algorithms can process large volumes of information, current and historical, to find patterns.
(3) Innocuous data input combined with publically available information can give the AI algorithms what is considered forbidden insider information.
(4) Traders don't necessarily understand how the AI algorithms works.
(5) If the traders don't understand how the AI algorithms works, they can't be accused of insider trading.
(6) The traders can never be prosecuted and gain unfair advantages.
-----
(7) [Insider Trading]: AI algorithms lead to unfair advantages in stock trading.


<Market Volatility>

(1) AI algorithms are capable of processing large volumes of information and trade at a high speed.
(2) High-speed trading can lead to market volatility.
(3) Market volatility can lead to huge losses.
(4) Huge losses can lead to bankrupcy.
-----
(5) [Volatility]: Market volatility can have dangerous results.


<High-Speed Trading>

(1) [Volatility]
(2) High-speed trading can lead to market volatility.
(3) AI algorithms are capable of processing large volumes of information and trade at a high speed.
-----
(4) [High-Speed]: AI algorithms can have dangerous results for the economy.




