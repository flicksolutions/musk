===
model:
    mode: strict
===

# Autonomous Weapons { isGroup: true }

<autonomous weapons> { Quelle: dochertyLosingHumanityCase2012, p.4 }

(1) [kein Mitgefühl]: autonome Waffensysteme haben keine menschlichen Emotionen oder die Fähigkeit zu Mitgefühl.
(2) [Mitgefühl hilft]: menschliche Emotionen und die Fähigkeit zu Mitgefühl vermindern das Töten von Zivilisten.
----
(3) [Zivile Verluste]: Die Entwicklung von autonomen Waffensystemen erhöht die Chance von Zivilisten in einem bewaffneten Konflikt verletzt oder getötet zu werden.


<bewaffneter Konflikt wahrscheinlicher> { Quelle: dochertyLosingHumanityCase2012, p.4 }

(1) Wenn ein verkleinertes Risiko besteht, eigene Truppen zu verlieren, wird ein bewaffneter Konflikt wahrscheindlicher.
(2) Die Ersetzung von Frontsoldaten durch Roboter verkleinert das Risiko, eigene Truppen zu verlieren.
----
(3) Ein bewaffneter Konflikt ist wahrscheinlicher.
(4) In einem bewaffneten Konflikt gibt es immer auch zivile Verluste.
----
(5) [Zivile Verluste]

<Verantwortlichkeit> { Quelle: dochertyLosingHumanityCase2012, p.4 }

(1) Wenn eine AI ein Gesetz bricht, gibt es drei Möglichkeiten, wer verantwortlich sein kann: Die AI, der Hersteller, der Softwareprogrammierer oder der militärische Befehlshaber.
(2) Hersteller, Softwareprogrammierer oder militärischen Befehlshaber für Verantwortlich zu erklären, wäre unfair und schwer.
(3) Die AI ist nicht bestrafbar.
----
(4) Die Verantwortung für ein Missverhalten der AI ist ungeklärt oder schwer zurückzuverfolgen.
(5) Wenn die Verantwortung ungeklärt ist oder schwer zurückzuverfolgen, ist vergeltende Gerechtigkeit nicht möglich und man könnte weniger gut vor Menschenrechtsverletzungen abhalten.
----
(6) [retributive Justice]: Vergeltende Gerechtigkeit ist nicht möglich und vor Menschenrechtsverletzungen kann weniger gut abgehalten werden.


<Tod ist Gefahr>

(1) [Zivile Verluste]
(2) [retributive Justice]
(3) Wenn mehr Zivilisten in einem bewaffneten Konflikt getötet werden und Menschenrechtsverletzungen weniger gut verhindert werden können, dann ergibt sich eine Gefahr für die Gesellschaft.
----
(4) [KI-Entwicklung ist gefährlich]


# Optimizing AI’s Economic Impact { isGroup: true }

<falsches Messinstrument> { Quelle: russellResearchPrioritiesRobust2015, p. 107 & mokyrSecularStagnationNot2014 }

(1) [moralische AI]: AI wird in den Markt eingeführt.
(2) Die Messinstrumente können einen Markt mit AI nicht korrekt erfassen.
(3) Wenn die Messinstrumente einen Markt nicht erfassen können, werden falsche Gesetze verabschiedet.
(4) Wenn falsche Gesetze verabschiedet werden, ist dies eine Gefahr für die Gesellschaft.
----
(5) [KI-Entwicklung ist gefährlich]

<Technological Unemployment> { Quellen:  brynjolfssonSecondMachineAge2014, p. 153 & mokyrSecularStagnationNot2014 }

(1) Technische Entwicklungen führen zu Jobverlusten.
(2) Jobverluste werden nach einer gewissen Zeitdauer *t* ausgeglichen, weil die technischen Entwicklungen neue Berufsfelder entstehen lassen.
(3) Durch die Entwicklung von AI wird die technologische Entwicklung derart beschleunigt, dass *t* immer weiter wächst.
----
(4) Die Entwicklung von AI führt zu permanenten Jobverlusten. - *t* geht gegen unendlich.
(5) Das Einkommen eines grossen Teils der Bevölkerung hängt von ihrem Job ab.
----
(6) [Armut]: Die Entwicklung von AI führt zu extremer Armut bei einem grossen Teil der Bevölkerung.

<Android-Experiment> { Quellen: brynjolfssonSecondMachineAge2014 }

(1) [moralische AI]: Es wird Androiden (AI mit einem Körper) geben.
(2) Menschen werden nur angestellt, wenn sie Produkte am kosteneffektivsten im Vergleich zu anderen Methoden herstellen können.
(3) Androiden können Produkte kosteneffektiver herstellen als Menschen.
----
(4) Es werden nur noch Androiden und keine Menschen mehr angestellt werden.
(5) Ein grosser Teil der Bevölkerung erwirtschaftet sein komplettes Einkommen aus ihrer Anstellung.
----
(6) [Armut]


<Power Law> { Quellen: brynjolfssonSecondMachineAge2014, chap. 10 & mokyrSecularStagnationNot2014 }

(1) [Potenzgesetz]: Wenn die Nachfrage nach einem Produkt von einem einzelnen Anbieter befriedigt werden kann, folgt die Verteilung des Gewinns an dem Produkt einem Potenzgesetz.
(2) [dig. Nachfrage]: Ein Anbieter von digitalen Produkten kann theoretisch die ganze Nachfrage nach einem Produkt befriedigen.
(3) [Vernetzung]: Wenn die Welt mehr vernetzt ist, werden Märkte vergrössert und Produkte haben mehr Konkurrenz.
(4) [Vernetzung Pot.]: Wenn Märkte grösser werden, erhalten weniger Produkte grössere Gewinnanteile. Die Verteilung des Gewinns folgt also eher einem Potenzgesetz.
(5) Die Digitalisierung führt zu einer grösseren Vernetzung der Welt.
(6) Netzwerke und Standards fördern ein Potenzgesetz.
(7) Die Digitalisierung führt zu mehr Netzwerken und Standards.
----
(8) Die Digitalisierung verändert die ökonomischen Prozesse so, dass die Verteilung des Reichtums einem Potenzgesetz ([Pareto-Verteilung](https://www.youtube.com/watch?v=fCn8zs912OE)) folgt und nicht mehr einer Normalverteilung (Glockenkurve).
Wenn die Verteilung von Reichtum einem Potenzgesetz folgt, wird Reichtum konzentriert.
(9) Durch die Konzentration von Reichtum wird ein positiver Feedback-Loop in Gang gesetzt, der den Effekt (Potenzgesetz) noch verstärkt brynjolfssonSecondMachineAge2014, p. 135
----
(10) [Wirtschaft folgt Potenzgesetz]: Die Verteilung von Reichtum folgt verstärkt einem Potenzgesetz.


<AI macht analog zu digital>

(1) [Potenzgesetz]
(2) [dig. Nachfrage]
(3) Durch die Entwicklung von AI (z.B. in Robotern) werden analoge Dienstleistungen zu digitalen. → Somit kann die ganze Nachfrage von einem Anbieter befriedigt werden.
----
(4) [Wirtschaft folgt Potenzgesetz]


<AI vergrössert Märkte>

(1) [Vernetzung]
(2) [Vernetzung Pot.:]
(3) Durch die Entwicklung von AI werden weitere Märkte vernetzt (globalisiert). (z.B. Bessere automatische Übersetzung)
----
(4) [Wirtschaft folgt Potenzgesetz]

<strong bounty> { Quellen:  brynjolfssonSecondMachineAge2014, p. 143}

(1) [Arme profitieren]: Bei einer Wirtschaft die nach einem Potenzgesetz funktioniert profitieren auch die Teilnehmer der Wirtschaft, die am wenigsten Verdienen.
(2) [Wirtschaft folgt Potenzgesetz]
(3) Wenn die Wirtschaft einem Potenzgesetz folgt und auch die Teilnehmer der Wirtschaft, die am wenigsten verdienen, profitieren, dann ist die Wirtschaft nach einem Potenzgesetz richtig.
----
(4) [Potenzgesetz gut]: Die Wirtschaft nach einem Potenzgesetz ist richtig.


<Potenzgesetz ist schlecht> { Quellen:  brynjolfssonSecondMachineAge2014, p. 144 }

(1) Das bereinigte Median- Einkommen und -Gesamtvermögen in den USA ist heute kleiner als 1999.
(2) [Einkommen klein, dann kein Profit]: Wenn das bereinigte Median- Einkommen und -Gesamtvermögen in den USA kleiner als 1999 ist, dann profitieren die Teilnehmer der Wirtschaft, die am wenigsten Verdienen nicht von einer Wirtschaft, die nach einem Potenzgesetz funktioniert.
----
(3) [Arme profitieren nicht]: Bei einer Wirtschaft die nach einem Potenzgesetz funktioniert profitieren die Teilnehmer der Wirtschaft, die am wenigsten Verdienen nicht.
    - [Arme profitieren]


<Potenzgesetz ist schlecht2> { Quellen:  brynjolfssonSecondMachineAge2014, p. 146 }

(1) Lebenswichtige Produkte werden teurer. (Miete, Gesundheitsversorgung und Bildungskosten)
(2) Die hälfte der Amerikaner haben kein finanzielles Polster.
(3) Die Armutsrate, der Zugang zum Gesundheitssystem und die Rate der Unterbeschäftigten sind hoch.
(4) Soziale Mobilität ist klein.
(5) Soziale Mobilität, Armutsrate, Zugang zum Gesundheitssystem, die Rate der Unterbeschäftigten, finanzielle Polster und der Preis lebenswichtiger Produkte sind wichtige Indikatoren für den Zustand der Ärmeren Bevölkerung.
(6) Wenn die Indikatoren für den Zustand der ärmeren Bevölkerung schlecht sind, dann profitiert die ärmere Bevölkerung nicht von einer Wirtschaft, die einem Powerlaw folgt.
----
(7) [Arme profitieren nicht]


<Bedingung falsch> { Quellen:  brynjolfssonSecondMachineAge2014, p. 146 }

(1) Wenn es andere wichtige Faktoren gibt, von denen die Teilnehmer der Wirtschaft, die am wenigsten Verdienen profitieren als das bereinigte Median- Einkommen und -Gesamtvermögen, dann ist @[Einkommen klein, dann kein Profit] falsch.
(2) Ungemessene Preisreduktionen und qualitative Verbesserungen sind Faktoren, von denen die Teilnehmer der Wirtschaft, die am wenigsten Verdienen, profitieren.
----
(3) [Einkommen klein, dann kein Profit-]: @[Einkommen klein, dann kein Profit] ist falsch.
    - [Einkommen klein, dann kein Profit]


# Computer Science Research for Robust AI

[Robustheit]: AI ist nicht robust und deshalb nicht sicher russellResearchPrioritiesRobust2015, p. 107

[Verifikation]: Ganze AI-Agents sind nicht formal verifizierbar:
we lack the formal algebra to properly define, explore,
and rank the space of designs. Perhaps the most salient difference between verification of traditional software and verification of AI systems is that the correctness of traditional software is defined with respect to a fixed and known machine model, whereas AI systems — especially robots and other embodied systems — operate in environments that are at best partially known by the system designer. russellResearchPrioritiesRobust2015, p. 108
Das Ganze gestaltet sich noch schwerer bei AI die learning Algorithmen anwendet.

[Validity]: Unsere Anforderungen an die AI können falsch formuliert sein und so könnten sich AI-Agents unpassend verhalten.
> Such specification errors are ubiquitous in software
> verification, where it is commonly observed that
> writing correct specifications can be harder than writing
> correct code. Unfortunately, it is not possible to
> verify the specification: the notions of beneficial and
> desirable are not separately made formal, so one cannot
straightforwardly prove that satisfying ψ necessarily
leads to desirable behavior and a beneficial
agent.
 russellResearchPrioritiesRobust2015, p. 108


<Vorteile verhindern Moratorium> { Quellen: wallachMoralMachinesTeaching2008, p.5, chap. 3 }

(1) AIs, die moralisch-relevante Entscheidungen treffen müssen, können grosse Vorteile bringen.
(2) Der Markt und die Politik verlangen nach diesen Vorteilen.
(3) Die Forschung beugt sich dem Willen des Marktes und der Politik
----
(4) [moralische AI]: AI, die moralisch relevante Entscheidungen treffen, werden entwickelt.

<schlechte Entscheide>

(1) Wenn bei der Entwicklung von AI die Fähigkeit moralische Entscheidungen vernachlässigt wird, trifft die AI schlechte moralische Entscheide.
(2) Bei der Entwicklung von AI wird die Fähigkeit moralische Entscheidungen zu treffen vernachlässigt.
-----
(3) [schlechte Entscheide]: AI treffen moralisch schlechte Entscheidungen.

<AI trifft schlechte Entscheide>

(1) [moralische AI]
(2) [schlechte Entscheide]
-----
(3) Es werden AIs entwickelt, die moralisch schlechte Entscheidungen treffen.

<Decision Support Tools> { Quellen: wallachMoralMachinesTeaching2008, p. 9 & wallachMoralMachinesTeaching2008, chap. 3 }

(1) DST unterstützen Menschen dabei, wichtige, folgenreiche Entscheidungen zu treffen.
(2) Wenn DST gut funktionieren, fangen Menschen an, wichtige, folgenreiche Entscheidungen nicht mehr selbst zu treffen, sondern überlassen die Entscheidungen den DST.
----
(3) [DST]: Wenn DST gut funktionieren, kontrollieren sie selbst Entscheidungsprozesse.

<schlecht und weitreichend>

(1) [DST]
(2) [schlechte Entscheide]
----
(3) wichtige, folgenreiche Entscheidungen werden moralisch schlecht gefällt.

<Cyberattacken> { Quellen: russellResearchPrioritiesRobust2015, p. 109 }

(1) AI wird Sicherheitsrelevante Aufgaben übernehmen.
(2) Wenn etwas sicherheitsrelevante Aufgaben übernimmt, bietet es Angriffsfläche für Cyberattacken.
(3) AI wird in Cyberattacken eingesetzt.
----
(4) AI sind sowohl Ziel als auch Ursprung von Cyberattacken.
(5) Cyberattacken sind eine Gefahr für die Gesellschaft.
----
(6) [KI-Entwicklung ist gefährlich]: AI ist eine Gefahr für die Gesellschaft.
