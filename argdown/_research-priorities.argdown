===
model:
    mode: strict
===

# Short-Term

## Law and Ethics

### Autonomous Weapons

[nicht-verboten]: Der Einsatz autonomer Waffen ist durch kein international bindendes Instrument verboten (Docherty 2012)
Auf nationaler Ebene existieren keine Gesetze, welche Entwicklung und Einsatz autonomer Waffen verbieten (Docherty 2012)

# Optimizing AI’s Economic Impact

Hier finden sich vorallem 2 Paper die Gefahren aufzeigen. Es geht dabei vor allem darum, dass dem Grossteil der Gesellschaft die Arbeitslosigkeit droht.

## Mokyr, J. 2014. Secular Stagnation? Not in Your Life.

<falsches Messinstrument> { Quelle: @russellResearchPrioritiesRobust2015, p. 107 & @mokyrSecularStagnationNot2014 }

(1) [moralische AI]: AI wird in den Markt eingeführt.
(2) Die Messinstrumente können einen Markt mit AI nicht korrekt erfassen.
(3) Wenn die Messinstrumente einen Markt nicht erfassen können, werden falsche Gesetze verabschiedet.
(4) Wenn falsche Gesetze verabschiedet werden, ist dies eine Gefahr für die Gesellschaft.
----
(5) [KI-Entwicklung ist gefährlich]

## The Second Machine Age: Work, Progress and Prosperity in a Time of Brilliant Technologies

<Power Law> { Quellen: brynjolfssonSecondMachineAge2014, chap. 10 & mokyrSecularStagnationNot2014 }

(1) "Normale" Wirtschaft folgt einer normalen Verteilung, weil die Nachfrage nicht von einem Anbieter befriedigt werden kann.
(2) Ein Anbieter von digitalen Produkten kann theoretisch die ganze Nachfrage nach einem Produkt befriedigen.
(3) Durch die Vernetzung der Welt (globaler Markt), können alle bestehenden Produkte verglichen werden, alle Produkte müssen also mit dem Besten konkurrieren. Dies führt zu einem Potenzgesetz.
(4) Die Digitalisierung führt zu einer grösseren Vernetzung der Welt.
(5) Netzwerke und Standards werden wichtiger in einer digitalen Welt.
(6) Netzwerke und Standards fördern ein Potenzgesetz.
----
(7) Die Digitalisierung verändert die ökonomischen Prozesse so, dass die Verteilung des Reichtums einem Potenzgesetz ([Pareto-Verteilung](https://www.youtube.com/watch?v=fCn8zs912OE)) folgt und nicht mehr einer Normalverteilung (Glockenkurve).
(8) Durch die Konzentration von Reichtum wird ein positiver Feedback-Loop in Gang gesetzt, der den Effekt (Potenzgesetz) noch verstärkt brynjolfssonSecondMachineAge2014, p. 135
(9) Durch die Entwicklung von AI werden weitere Märkte vernetzt (globalisiert). (z.B. Übersetzung)
(10) Durch die Entwicklung von AI (Roboter) werden analoge Dienstleistungen zu digitalen. → Somit kann die ganze Nachfrage von einem Anbieter befriedigt werden.
----
(11) Die Entwicklung von AI führt dazu, dass die Wirtschaft noch stärker einem Potenzgesetz folgt.
(x)
----
(12) [Armut]

<Technological Unemployment> { Quellen:  brynjolfssonSecondMachineAge2014, p. 153 & mokyrSecularStagnationNot2014 }

(1) Technische Entwicklungen führen zu Jobverlusten.
(2) Jobverluste werden nach einer gewissen Zeitdauer *t* ausgeglichen, weil die technischen Entwicklungen neue Berufsfelder entstehen lassen.
(3) Durch die Entwicklung von AI wird die technologische Entwicklung derart beschleunigt, dass *t* immer weiter wächst.
----
(4) Die Entwicklung von AI führt zu permanenten Jobverlusten. - *t* geht gegen unendlich.
(5) Das Einkommen eines grossen Teils der Bevölkerung hängt von ihrem Job ab.
----
(6) [Armut]: Die Entwicklung von AI führt zu extremer Armut bei einem grossen Teil der Bevölkerung.

<Android-Experiment> { Quellen: brynjolfssonSecondMachineAge2014 }

(1) [moralische AI]: Es wird Androiden (AI mit einem Körper) geben.
(2) Menschen werden nur angestellt, wenn sie Produkte am kosteneffektivsten im Vergleich zu anderen Methoden herstellen können.
(3) Androiden können Produkte kosteneffektiver herstellen als Menschen.
----
(4) Es werden nur noch Androiden und keine Menschen mehr angestellt werden.
(5) Ein grosser Teil der Bevölkerung erwirtschaftet sein komplettes Einkommen aus ihrer Anstellung.
----
(6) [Armut]

# Computer Science Research for Robust AI

[Robustheit]: AI ist nicht robust und deshalb nicht sicher russellResearchPrioritiesRobust2015, p. 107

[Verifikation]: Ganze AI-Agents sind nicht formal verifizierbar:
we lack the formal algebra to properly define, explore,
and rank the space of designs. Perhaps the most salient difference between verification of traditional software and verification of AI systems is that the correctness of traditional software is defined with respect to a fixed and known machine model, whereas AI systems — especially robots and other embodied systems — operate in environments that are at best partially known by the system designer. russellResearchPrioritiesRobust2015, p. 108
Das Ganze gestaltet sich noch schwerer bei AI die learning Algorithmen anwendet.

[Validity]: Unsere Anforderungen an die AI können falsch formuliert sein und so könnten sich AI-Agents unpassend verhalten.
> Such specification errors are ubiquitous in software
> verification, where it is commonly observed that
> writing correct specifications can be harder than writing
> correct code. Unfortunately, it is not possible to
> verify the specification: the notions of beneficial and
> desirable are not separately made formal, so one cannot
straightforwardly prove that satisfying ψ necessarily
leads to desirable behavior and a beneficial
agent.
 russellResearchPrioritiesRobust2015, p. 108


<Vorteile verhindern Moratorium> { Quellen: wallachMoralMachinesTeaching2008, p.5, chap. 3 }

(1) AIs, die moralisch-relevante Entscheidungen treffen müssen, können grosse Vorteile bringen.
(2) Der Markt und die Politik verlangen nach diesen Vorteilen.
(3) Die Forschung beugt sich dem Willen des Marktes und der Politik
----
(4) [moralische AI]: AI, die moralisch relevante Entscheidungen treffen, werden entwickelt.

<schlechte Entscheide>

(1) Wenn bei der Entwicklung von AI die Fähigkeit moralische Entscheidungen vernachlässigt wird, trifft die AI schlechte moralische Entscheide.
(2) Bei der Entwicklung von AI wird die Fähigkeit moralische Entscheidungen zu treffen vernachlässigt.
-----
(3) [schlechte Entscheide]: AI treffen moralisch schlechte Entscheidungen.

<AI trifft schlechte Entscheide>

(1) [moralische AI]
(2) [schlechte Entscheide]
-----
(3) Es werden AIs entwickelt, die moralisch schlechte Entscheidungen treffen.

<Decision Support Tools> { Quellen: wallachMoralMachinesTeaching2008, p. 9 & wallachMoralMachinesTeaching2008, chap. 3 }

(1) DST unterstützen Menschen dabei, wichtige, folgenreiche Entscheidungen zu treffen.
(2) Wenn DST gut funktionieren, fangen Menschen an, wichtige, folgenreiche Entscheidungen nicht mehr selbst zu treffen, sondern überlassen die Entscheidungen den DST.
----
(3) [DST]: Wenn DST gut funktionieren, kontrollieren sie selbst Entscheidungsprozesse.

<schlecht und weitreichend>

(1) [DST]
(2) [schlechte Entscheide]
----
(3) wichtige, folgenreiche Entscheidungen werden moralisch schlecht gefällt.

<Cyberattacken> { Quellen: russellResearchPrioritiesRobust2015, p. 109 }

(1) AI wird Sicherheitsrelevante Aufgaben übernehmen.
(2) Wenn etwas sicherheitsrelevante Aufgaben übernimmt, bietet es Angriffsfläche für Cyberattacken.
(3) AI wird in Cyberattacken eingesetzt.
----
(4) AI sind sowohl Ziel als auch Ursprung von Cyberattacken.
(5) Cyberattacken sind eine Gefahr für die Gesellschaft.
----
(6) [KI-Entwicklung ist gefährlich]: AI ist eine Gefahr für die Gesellschaft.

# Long-Term

## Verification

[selbst-veränderung]: If humanity creates an agent that attains superintelligence through a sequence of successive self-improvments, the resulting system may be quite different from the initial verified system. (Fallenstein and Soares 2014)

## Control

[understanding of nature]: With increasing intelligence may come not merely a quantitative improvement in the ability to attain the same old goals, but a qualitatively different understanding of the nature of reality that reveals the old goals to be misguided, meaningless or even undefined. (Tegmark 2015)
