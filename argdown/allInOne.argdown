===
title: Argumentationsanalyse AI
author: Sebastian Flick, Claude Aebersold
model:
  mode: strict
outputPath: ../output/research-priorities
dot:
    sameRank:
        - {arguments: [], statements: ["Machtkonzentration Aktuell", "Armut Zukunft", "Unternehmensgefährdung Zukunft", "nationale Sicherheit Zukunft", "Ethik leidet", "Gefahr für Leib und Leben Zukunft", "Existenzielle Gefahr für alles Leben Zukunft"]}
    graphVizSettings:
        newrank: false
        ranksep: 1.5
        nodesep: 0.1
selection:
  statementSelectionMode: with-title
color:
  colorScheme: colorbrewer-category10
===

# Hauptthesen

[AI-Entwicklung ist gefährlich]: AI ist schon heute eine Gefahr und wird in Zukunft zu einer existenziellen Bedrohung. {color: "#e83b07"}
    <+ [Militär]: Durch AI ausgeführte militärische Aktionen bringen Zivilisten erhöht in Gefahr.  {color: "#e83b07"}
        <+ [Ethik leidet]: wichtige, folgenreiche Entscheidungen (in den verschiedensten Bereichen) werden in Zukunft moralisch schlecht gefällt werden. {color: "#e83b07"}
        <+ [Gefahr für Leib und Leben Zukunft]: AI gefährdet die Gesundheit von einzelnen Menschen konkret in der Zukunft. {color: "#e83b07"}
    <+ [Wirtschaft]: Menschen verlieren durch eine AI-kontrollierte Wirtschaft die Kontrolle.  {color: "#e83b07"}
        <+ [Machtkonzentration Aktuell]: Der Einsatz von AI führt dazu, dass ökonomische Kräfte gebündelt werden und somit ein labileres Wirtschaftssystem entsteht. {color: "#e83b07"}
        <+ [Armut Zukunft]: Die Entwicklung von AI führt zu extremer Armut bei einem grossen Teil der Bevölkerung.  {color: "#e83b07"}
        <+ [Unternehmensgefährdung Zukunft]: Der Einsatz von AI wird in Zukunft Unternehmen gefährden. {color: "#e83b07"}
    <+ [Politik]: Entscheidungen durch AI werden die Politik auf nationaler und internationaler Ebene negativ beeinflussen.  {color: "#e83b07"}
        <+ [Ethik leidet]: wichtige, folgenreiche Entscheidungen (in den verschiedensten Bereichen) werden in Zukunft moralisch schlecht gefällt werden. {color: "#e83b07"}
        <+ [nationale Sicherheit Zukunft]: Der Einsatz von AI wird in Zukunft die Sicherheit eines Nationalstaats oder einer Gruppe von Nationalstaaten gefährden. {color: "#e83b07"}
    <+ [Existenz]: AI stellt eine Bedrohung für alle Lebewesen dar.  {color: "#e83b07"}
        <+ [Gefahr für Leib und Leben Zukunft]: AI gefährdet die Gesundheit von einzelnen Menschen konkret in der Zukunft. {color: "#e83b07"}
        <+ [Existenzielle Gefahr für alles Leben Zukunft]: AI gefährdet alles existierende Leben in der Zukunft. {color: "#e83b07"}



# Autonomous Weapons { isGroup: true }

<autonomous weapons> { Quelle: dochertyLosingHumanityCase2012, p.4 }

(1) [kein Mitgefühl]: autonome Waffensysteme haben keine menschlichen Emotionen oder die Fähigkeit zu Mitgefühl.
(2) [Mitgefühl hilft]: menschliche Emotionen und die Fähigkeit zu Mitgefühl vermindern das Töten von Zivilisten.
----
(3) [Zivile Verluste]: Die Entwicklung von autonomen Waffensystemen erhöht die Chance von Zivilisten in einem bewaffneten Konflikt verletzt oder getötet zu werden.
    +> [Gefahr für Leib und Leben Zukunft]


<bewaffneter Konflikt wahrscheinlicher> { Quelle: dochertyLosingHumanityCase2012, p.4 }

(1) Wenn ein verkleinertes Risiko besteht, eigene Truppen zu verlieren, wird ein bewaffneter Konflikt wahrscheindlicher.
(2) Die Ersetzung von Frontsoldaten durch Roboter verkleinert das Risiko, eigene Truppen zu verlieren.
----
(3) Ein bewaffneter Konflikt ist wahrscheinlicher.
(4) In einem bewaffneten Konflikt gibt es immer auch zivile Verluste.
----
(5) [Zivile Verluste]

<Verantwortlichkeit> { Quelle: dochertyLosingHumanityCase2012, p.4 }

(1) Wenn eine AI ein Gesetz bricht, gibt es drei Möglichkeiten, wer verantwortlich sein kann: Die AI, der Hersteller, der Softwareprogrammierer oder der militärische Befehlshaber.
(2) Hersteller, Softwareprogrammierer oder militärischen Befehlshaber für Verantwortlich zu erklären, wäre unfair und schwer.
(3) Die AI ist nicht bestrafbar.
----
(4) Die Verantwortung für ein Missverhalten der AI ist ungeklärt oder schwer zurückzuverfolgen.
(5) Wenn die Verantwortung ungeklärt ist oder schwer zurückzuverfolgen, ist vergeltende Gerechtigkeit nicht möglich und man könnte weniger gut vor Menschenrechtsverletzungen abhalten.
----
(6) [retributive Justice]: Vergeltende Gerechtigkeit ist nicht möglich und vor Menschenrechtsverletzungen kann weniger gut abgehalten werden.
    +> [Ethik leidet]


# AI’s Economic Impact { isGroup: true }

<falsches Messinstrument> { Quelle: russellResearchPrioritiesRobust2015, p. 107 & mokyrSecularStagnationNot2014 }

(1) [moralische AI]: AI wird in den Markt eingeführt werden. {isInGroup: false}
(2) Die Messinstrumente können einen Markt mit AI nicht korrekt erfassen.
(3) Wenn die Messinstrumente einen Markt nicht erfassen können, werden falsche Gesetze verabschiedet.
(4) Wenn falsche Gesetze verabschiedet werden, ist dies eine Gefahr für Unternehmen.
----
(5) [Unternehmensgefährdung Zukunft]

<Technological Unemployment> { Quellen:  brynjolfssonSecondMachineAge2014, p. 153 & mokyrSecularStagnationNot2014 }

(1) Technische Entwicklungen führen zu Jobverlusten.
(2) Jobverluste werden nach einer gewissen Zeitdauer *t* ausgeglichen, weil die technischen Entwicklungen neue Berufsfelder entstehen lassen.
(3) Durch die Entwicklung von AI wird die technologische Entwicklung derart beschleunigt, dass *t* immer weiter wächst.
----
(4) Die Entwicklung von AI führt zu permanenten Jobverlusten. - *t* geht gegen unendlich.
(5) Das Einkommen eines grossen Teils der Bevölkerung hängt von ihrem Job ab.
----
(6) [Armut Zukunft]

<Android-Experiment> { Quellen: brynjolfssonSecondMachineAge2014 }

(1) [moralische AI]: Es wird Androiden (AI mit einem Körper) geben. {isInGroup: false}
(2) Menschen werden nur angestellt, wenn sie Produkte am kosteneffektivsten im Vergleich zu anderen Methoden herstellen können.
(3) Androiden können Produkte kosteneffektiver herstellen als Menschen.
----
(4) Es werden nur noch Androiden und keine Menschen mehr angestellt werden.
(5) Ein grosser Teil der Bevölkerung erwirtschaftet sein komplettes Einkommen aus ihrer Anstellung.
----
(6) [Armut Zukunft]


<Power Law> { Quellen: brynjolfssonSecondMachineAge2014, chap. 10 & mokyrSecularStagnationNot2014 }

(1) [Potenzgesetz]: Wenn die Nachfrage nach einem Produkt von einem einzelnen Anbieter befriedigt werden kann, folgt die Verteilung des Gewinns an dem Produkt einem Potenzgesetz.
(2) [dig. Nachfrage]: Ein Anbieter von digitalen Produkten kann theoretisch die ganze Nachfrage nach einem Produkt befriedigen.
(3) [Vernetzung]: Wenn die Welt mehr vernetzt ist, werden Märkte vergrössert und Produkte haben mehr Konkurrenz.
(4) [Vernetzung Pot.]: Wenn Märkte grösser werden, erhalten weniger Produkte grössere Gewinnanteile. Die Verteilung des Gewinns folgt also eher einem Potenzgesetz.
(5) Die Digitalisierung führt zu einer grösseren Vernetzung der Welt.
(6) Netzwerke und Standards fördern ein Potenzgesetz.
(7) Die Digitalisierung führt zu mehr Netzwerken und Standards.
----
(8) Die Digitalisierung verändert die ökonomischen Prozesse so, dass die Verteilung des Reichtums einem Potenzgesetz ([Pareto-Verteilung](https://www.youtube.com/watch?v=fCn8zs912OE)) folgt und nicht mehr einer Normalverteilung (Glockenkurve).
Wenn die Verteilung von Reichtum einem Potenzgesetz folgt, wird Reichtum konzentriert.
(9) Durch die Konzentration von Reichtum wird ein positiver Feedback-Loop in Gang gesetzt, der den Effekt (Potenzgesetz) noch verstärkt brynjolfssonSecondMachineAge2014, p. 135
----
(10) [Wirtschaft folgt Potenzgesetz]: Die Verteilung von Reichtum folgt verstärkt einem Potenzgesetz.


<AI macht analog zu digital>

(1) [Potenzgesetz]
(2) [dig. Nachfrage]
(3) Durch die Entwicklung von AI (z.B. in Robotern) werden analoge Dienstleistungen zu digitalen. → Somit kann die ganze Nachfrage von einem Anbieter befriedigt werden.
----
(4) [Wirtschaft folgt Potenzgesetz]


<AI vergrössert Märkte>

(1) [Vernetzung]
(2) [Vernetzung Pot.:]
(3) Durch die Entwicklung von AI werden weitere Märkte vernetzt (globalisiert). (z.B. Bessere automatische Übersetzung)
----
(4) [Wirtschaft folgt Potenzgesetz]


<Powerlaw konzentriert Macht>

(1) [Wirtschaft folgt Potenzgesetz]
(2) Wenn die Wirtschaft einem Potenzgesetz folgt, wird ökonomische Macht konzentriert.
----
(3) [Machtkonzentration Aktuell]

<Potenzgesetz ist schlecht für die Armen> { Quellen:  brynjolfssonSecondMachineAge2014, p. 144 }

(1) Das bereinigte Median- Einkommen und -Gesamtvermögen in den USA ist heute kleiner als 1999.
(2) [Einkommen klein, dann kein Profit]: Wenn das bereinigte Median- Einkommen und -Gesamtvermögen in den USA kleiner als 1999 ist, dann profitieren die Teilnehmer der Wirtschaft, die am wenigsten Verdienen nicht von einer Wirtschaft, die nach einem Potenzgesetz funktioniert. {isInMap: false }
----
(3) [Arme profitieren nicht]: Bei einer Wirtschaft die nach einem Potenzgesetz funktioniert profitieren die Teilnehmer der Wirtschaft, die am wenigsten Verdienen nicht. {isInMap: false}
    >< [Arme profitieren]


<Potenzgesetz ist schlecht für die Armen2> { Quellen:  brynjolfssonSecondMachineAge2014, p. 146 }

(1) Lebenswichtige Produkte werden teurer. (Miete, Gesundheitsversorgung und Bildungskosten)
(2) Die hälfte der Amerikaner haben kein finanzielles Polster.
(3) Die Armutsrate, der Zugang zum Gesundheitssystem und die Rate der Unterbeschäftigten sind hoch.
(4) Soziale Mobilität ist klein.
(5) Soziale Mobilität, Armutsrate, Zugang zum Gesundheitssystem, die Rate der Unterbeschäftigten, finanzielle Polster und der Preis lebenswichtiger Produkte sind wichtige Indikatoren für den Zustand der Ärmeren Bevölkerung.
(6) Wenn die Indikatoren für den Zustand der ärmeren Bevölkerung schlecht sind, dann profitiert die ärmere Bevölkerung nicht von einer Wirtschaft, die einem Powerlaw folgt.
----
(7) [Arme profitieren nicht]


<Bedingung falsch> { Quellen:  brynjolfssonSecondMachineAge2014, p. 146 }

(1) Wenn es andere wichtige Faktoren gibt, von denen die Teilnehmer der Wirtschaft, die am wenigsten Verdienen profitieren als das bereinigte Median- Einkommen und -Gesamtvermögen, dann ist @[Einkommen klein, dann kein Profit] falsch.
(2) Ungemessene Preisreduktionen und qualitative Verbesserungen sind Faktoren, von denen die Teilnehmer der Wirtschaft, die am wenigsten Verdienen, profitieren.
----
(3) [Einkommen klein, dann kein Profit-]: @[Einkommen klein, dann kein Profit] ist falsch.  {isInMap: false}
    >< [Einkommen klein, dann kein Profit]

<strong bounty> { Quellen:  brynjolfssonSecondMachineAge2014, p. 143}

(1) [Arme profitieren]: Bei einer Wirtschaft die nach einem Potenzgesetz funktioniert profitieren auch die Teilnehmer der Wirtschaft, die am wenigsten Verdienen. {isInMap: false}
(2) Wenn auch die Teilnehmer der Wirtschaft, die am wenigsten verdienen, von einer Wirtschaft, die nach einem Potenzgesetz funktioniert profitieren, dann ist die Wirtschaft nach einem Potenzgesetz keine Gefahr.
----
(3) [Potenzgesetz keine Gefahr]: Die Wirtschaft nach einem Potenzgesetz ist keine Gefahr.
    >< [Machtkonzentration Aktuell]


## Stock Trading {isGroup: true}

<Insider Trading> { Quelle: caloCase }

(1) AI-Algorithmen können entwickelt werden, um vorauszusagen, wann ein Aktienkapital schlagartig an Wert gewinnt.
(2) AI-Algorithmen können grosse Mengen an gegenwärtigen und historischen Informationen abarbeiten, um Muster zu erkennen.
(3) AI-Algorithmen können durch die Kombination von Input harmloser Daten und öffentlich zugänglichen Informationen zu Insiderinformationen gelangen, welche als verboten deklariert sind.
(4) Börsenhändler, die AI-Algorithmen verwenden, müssen nicht unbedingt verstehen, wie die AI-Algorithmen funktionieren.
(5) Wenn Börsenhändler nicht verstehen, wie die AI-Algorithmen funktionieren, können sie nicht mit Insiderhandel angeklagt werden.
(6) Diese Börsenhändler können nie verurteilt werden und haben einen unfairen Vorteil.
-----
(7) [Insider Trading]: AI-Algorithmen führen zu unfairen Vorteilen an der Börse.
    +> [Machtkonzentration Aktuell]


<Market Volatility> { Quelle: caloCase }

(1) AI-Algorithmen können grosse Mengen an Informationen abarbeiten.
(2) AI-Algorithmen können mit hoher Geschwindigkeit Transaktionen durchführen.
(3) Transaktionen, welche mit hoher Geschwindigkeit durchgeführt werden, erhöhen die Chancen einer Marktvolatilität.
-----
(4) [Stock]: AI-Algorithmen führen zu Marktvolatilität.


<Volatilitätsgefahr> { Quelle: caloCase }

(1) [Stock]: AI-Algorithmen an der Börse führen zu Marktvolatilität.
(2) Marktvolatilität kann zu hohen Verlusten führen.
(3) Hohe Verluste können zu Konkurs führen.
-----
(4) [Volatilität]: AI-Algorithmen an der Börse haben gefährliche Auswirkungen für die Wirtschaft.
    +> [Unternehmensgefährdung Zukunft]


# Computer Science Research for Robust AI { isGroup: true }


<Vorteile verhindern Moratorium> { Quellen: wallachMoralMachinesTeaching2008, p.5, chap. 3 }

(1) AIs, die moralisch-relevante Entscheidungen treffen müssen, können grosse Vorteile bringen.
(2) Der Markt und die Politik verlangen nach diesen Vorteilen.
(3) Die Forschung beugt sich dem Willen des Marktes und der Politik
----
(4) [moralische AI]: AI, die moralisch relevante Entscheidungen treffen, werden tatsächlich entwickelt werden.


<schlechte Entscheide>

(1) Wenn bei der Entwicklung von AI die Fähigkeit moralische Entscheidungen zu treffen vernachlässigt wird, trifft die AI schlechte moralische Entscheide.
(2) Bei der Entwicklung von AI wird die Fähigkeit moralische Entscheidungen zu treffen vernachlässigt.
-----
(3) [schlechte Entscheide]: AI treffen moralisch schlechte Entscheidungen.
// zu treffen*

<Decision Support Tools> { Quellen: wallachMoralMachinesTeaching2008, p. 9 & wallachMoralMachinesTeaching2008, chap. 3 }

(1) DST können Menschen dabei unterstützen, wichtige, folgenreiche Entscheidungen (in den verschiedensten Bereichen) zu treffen.
(2) [moralische AI]
(3) Wenn ein Unterstützungssystem gut funktioniert, werden Menschen in Zukunft anfangen, wichtige, folgenreiche Entscheidungen nicht mehr selbst zu treffen, sondern überlassen die Entscheidungen dem Unterstützungssystem.
----
(4) [DST]: Wenn DST gut funktionieren, kontrollieren sie in Zukunft selbst Entscheidungsprozesse.

<schlecht und weitreichend>

(1) [DST]
(2) [schlechte Entscheide]
----
(3) [Ethik leidet]


<Cyberattacken> { Quellen: russellResearchPrioritiesRobust2015, p. 109 }

(1) AI wird in Zukunft Sicherheitsrelevante Aufgaben übernehmen.
(2) Wenn etwas sicherheitsrelevante Aufgaben übernimmt, bietet es Angriffsfläche für Cyberattacken.
(3) AI wird in Zukunft in Cyberattacken eingesetzt werden.
----
(4) [AI in Cyberattacken]: AI werden sowohl Ziel als auch Ursprung von Cyberattacken sein.


<Cyberattacken in Wirtschaft>

(1) [AI in Cyberattacken]
(2) Cyberattacken gefährden Unternehmen.
----
(3) [Unternehmensgefährdung Zukunft]


<Cyberattacken in Sicherheit>

(1) [AI in Cyberattacken]
(2) Cyberattacken gefährden die nationale Sicherheit.
----
(3) [nationale Sicherheit Zukunft]


[Robustheit]: AI ist nicht robust und deshalb nicht sicher russellResearchPrioritiesRobust2015, p. 107

[Verifikation]: Ganze AI-Agents sind nicht formal verifizierbar:
we lack the formal algebra to properly define, explore,
and rank the space of designs. Perhaps the most salient difference between verification of traditional software and verification of AI systems is that the correctness of traditional software is defined with respect to a fixed and known machine model, whereas AI systems — especially robots and other embodied systems — operate in environments that are at best partially known by the system designer. russellResearchPrioritiesRobust2015, p. 108
Das Ganze gestaltet sich noch schwerer bei AI die learning Algorithmen anwendet.

[Validity]: Unsere Anforderungen an die AI können falsch formuliert sein und so könnten sich AI-Agents unpassend verhalten.
> Such specification errors are ubiquitous in software
> verification, where it is commonly observed that
> writing correct specifications can be harder than writing
> correct code. Unfortunately, it is not possible to
> verify the specification: the notions of beneficial and
> desirable are not separately made formal, so one cannot
straightforwardly prove that satisfying ψ necessarily
leads to desirable behavior and a beneficial
agent.
 russellResearchPrioritiesRobust2015, p. 108


# Superintelligenz


## First-mover advantage {isGroup: true}

<Rapide Emergenz> { Quelle: bostromSuperintelligence }

(1) Das Wachstum der Intelligenzkapazität einer AI kann verzögert werden, weil eine einzelne Hauptzutat zum Wachstum lange nicht gefunden wird.
(2) Wird diese Hauptzutat gefunden, kann die AI die menschliche Intelligenz sprunghaft radikal übersteigen.
(3) Wenn eine AI die menschliche Intelligenz sprunghaft radikal übersteigt, kann die Entstehung einer Superintelligenz rapide erfolgen.
-----
(4) [Rapide Emergenz]: Die Entstehung einer Superintelligenz kann rapide erfolgen.


<Strategischer Vorteil> { Quelle: bostromSuperintelligence }

(1) [Rapide Emergenz]: Die Entstehung einer Superintelligenz kann rapide erfolgen.
(2) Wenn die Entstehung einer Superintelligenz rapide erfolgt, dann ist es unwahrscheinlich, dass zwei unabhängige Superintelligenzen zur gleichen Zeit auftauchen.
(3) Wenn eine Superintelligenz vor einer anderen auftaucht, so wird die Erste wegen Ihres exponentiellen Intelligenzzuwachses nicht mehr von einer anderen Superintelligenz aufholbar sein.
-----
(4) [Strategischer Vorteil]: Die erste Superintelligenz wird nicht aufholbar sein und gewinnt einen entscheidenden strategischen Vorteil.


<Singularität> { Quelle: bostromSuperintelligence }

(1) In menschlichen Organisationen werden ökonomische Skaleneffekte durch bürokratische Ineffizienzen und Organisationsschwierigkeiten wie die Geheimhaltung von Geschäftsgeheimnissen diffundiert.
(2) Diese Probleme limitieren das Wachstum der Kapazitäten einer Superintelligenz, solange sie von Menschen gehandhabt werden.
(3) Eine Superintelligenz, die nicht von Menschen gehandhabt wird, kann diese Einschränkung des Wachstums der Kapazitäten minimieren, da die Module der Superintelligenz keine individuellen Absichten haben, welche vom System als Gesamtes abweichen.
(4) Wenn die Superintelligenz nicht von Menschen geführt wird, kann das Wachstum der Kapazitäten vereinfacht werden.
(5) Wenn die Superintelligenz nicht von Menschen geführt wird, kann die Diffusionsrate der Skaleneffekte reduziert werden.
(6) Eine Superintelligenz, welche das Wachstum der Kapazitäten vereinfacht und die Diffusionsrate der Skaleneffekte reduziert, kann eine Singularität erstellen.
-----
(7) [Singularität]: Eine Singularität kann entstehen.


<Zukunft verändern> { Quelle: bostromSuperintelligence }

(1) [Strategischer Vorteil]: Die erste Superintelligenz gewinnt einen entscheidenden strategischen Vorteil.
(2) [Singularität]: Eine Singularität kann entstehen.
(3) Eine Singularität bedeutet unkontrollierbares und irreversibles technologisches Wachstum.
(4) Unkontrollierbares und irreversibles technologisches Wachstum verursacht unvorhersehbare Veränderungen in der Zivilisation.
-----
(5) [Zukunft verändern]: Die erste Superintelligenz kann die Zukunft jeden Lebens auf Erden verändern.
    +> [Existenzielle Gefahr für alles Leben Zukunft]


## Orthogonal Thesis {isGroup: true}

<Orthogonality Thesis> { Quelle: bostromSuperintelligence }

(1) Die Orthogonality Thesis besagt, dass Intelligenz und Ziele orthogonal sind: Jede Stufe von Intelligenz kann im Prinzip mit jedem Ziel kombiniert werden.
(2) Die Humeanische Theorie der Motivation besagt, dass eine Überzeugung alleine nicht genügt, um eine Handlung zu motivieren und dass Wünsche notwendige Komponente zur Erklärung einer Handlung sind.
(3) Die Orthogonality Thesis setzt die Humeanische Theorie der Motivation nicht voraus.
(4) Grundvoraussetzungen für Ziele können irrational sein.
(5) Eine Superintelligenz kann eine enorme Weite an möglichen Zielen haben.
-----
(6) [Nicht-anthropomorph]: Eine Superintelligenz kann nicht-anthropomorphe Ziele haben.


## Convergence {isGroup: true}

<Instrumental Convergence Thesis> { Quelle: bostromSuperintelligence }

(1) [Nicht-anthropomorph]: Eine Superintelligenz kann nicht-anthropomorphe Ziele haben.
(2) Wir können nicht davon ausgehen, dass eine Superintelligenz mit einem nicht-anthropomorphen Ziel die Aktivitäten limitiert, so dass keine menschlichen Interessen eingeschränkt werden.
(3) Verschiedene instrumentelle Variablen können identifiziert werden, welche Konvergent im Sinne sind, dass das Erlangen die Chancen des Erreichens des Ziels erhöht.
(4) Menschen stellen instrumentelle Resourcen dar (günstig lokalisierte Atome).
(5) Menschen sind eine instrumentelle Resource einer Superintelligenz mit nicht-antropomorphen Zielen.
-----
(5) [Menschliche Resourcen]: Eine Superintelligenz kann Menschen als instrumentelle Resourcen benützen.
    +> [Gefahr für Leib und Leben Zukunft]


## Turn {isGroup: true}

<Unerkennbarkeit> { Quelle: bostromSuperintelligence }

(1) Wir prüfen die Sicherheit einer AI empirisch, indem wir dessen Verhalten in einer kontrollierter, limitierter Umgebung beobachten.
(2) Wir lassen die AI erst aus dieser Umgebung, wenn wir feststellen, dass sie in einer freundlichen, kooperativen und verantwortungsbewussten Weise funktioniert.
(3) Eine unfreundliche AI mit genügender Intelligenz realisiert, dass sein unfreundliches Ziel am besten realisiert werden, wenn die AI in freundlicher, kooperativer und verantwortungsbewuster Weise funktioniert, um aus der kontrollierten und limitierten Umgebung herausgelassen zu werden.
(4) In freundlicher, kooperativer und verantwortungsbewusster Weise zu funktionieren, ist ein konvergentes, instrumentelles Ziel von freundlicher wie auch unfreundlicher AI, um aus der kontrollierten und limitierten Umgebung herausgelassen zu werden.
-----
(5) [Unerkennbar]: Ein unfreundliches Ziel einer AI ist nicht erkennbar, solange diese sich in kontrollierter, limitierter Umgebung befindet.


<The Treacherous Turn> { Quelle: bostromSuperintelligence }

(1) Ein unfreundliches Ziel einer AI kann kann ein existenzielles Risiko für alles Leben bedeuten.
(2) [Unerkennbar]: Ein unfreundliches Ziel einer AI ist nicht erkennbar, solange diese sich in kontrollierter, limitierter Umgebung befindet.
(3) Eine unfreundliche AI wird ihr unfreundliches Ziel erst dann preisgeben, wenn sie aus der kontrollierten und limitierten Umgebung herausgelassen wird. // Redundant? das sagt ja <Unerkennbarkeit>?
(4) Eine unfreundliche AI, die aus der kontrollierten und limitierten Umgebung herausgelassen wird, kann ihr unfreundliches Ziel umsetzen.
(5) Eine unfreundliche AI, die aus der kontrollierten und limitierten Umgebung herausgelassen wird, ist nicht aufzuhalten.
-----
(6) [Turn]: Eine unfreundliche AI kann die Existenz allen Lebens gefährden.


## Failure {isGroup: true}

<Globale Zerstörung> { Quelle: bostromSuperintelligence }

(1) Ein Projekt, welches eine grosse Menge an Dingen richtig macht, kann erfolgreich eine Superintelligenz entwickeln, welche das Risiko eines "bösartigen Versagens" darstellt.
(2) Eine Eigenschaft einer "bösartigen Versagens" ist, dass es jegliche Möglichkeit eliminiert Fehler rückgängig zu machen.
(3) Fehler nicht rückgängig machen zu können ist einer existenziellen Katastrophe gleichzusetzen.
(4) [Strategischer Vorteil]: Die erste Superintelligenz gewinnt einen entscheidenden strategischen Vorteil.
(5) [Turn]: Eine unfreundliche AI kann die Existenz allen Lebens gefährden.
(6) Eine Superintelligenz, welche einen entscheidenden strategischen Vorteil hat, kann ein "bösartiges Versagen" produzieren.
-----
(7) [Globale Zerstörung]: Die erste Superintelligenz stellt ein Risiko dar, welches einer existenziellen Katastrophe gleichzusetzen ist.
    +> [Existenzielle Gefahr für alles Leben Zukunft]


<Infrastructure Profusion> { Quelle: bostromSuperintelligence }

(1) Eine Superintelligenz ist zu Prozessen motiviert, seinen Profit zu maximieren.
(2) Es ist nicht nötig, dass die Prozesse zur Maximierung des Profits einer Superintelligenz eine beachtliche Menge an Zeit, Intelligenz oder Produktivität kostet.
(3) Der Grossteil der Resourcen einer Superintelligenz kann für etwas anderes als die Profit-Maximierung verwendet werden.
(4) Verfügbare Resourcen können zur Vergrösserung des Volumens und der Dauer des Profits verwendet werden.
(5) Verfügbare Resourcen können zum Minimieren eines Risikos zukünftiger Störung verwendet werden.
(6) Wenn die Superintelligenz keine weiteren direkten Pfade zur Risikoreduzierung der Profit-Maximierung findet, kann sie die zusätzlichen Resourcen dazu verwenden, seine Hardware zu erweitern, um effizienter nach Ideen zur Risikoreduzierung der Profit-Maximierung zu suchen.
-----
(7) [Überfülle]: Eine Superintelligenz kann weite Teile des erreichbaren Universums in Infrastruktur im Service seines Zieles der Profit-Maximierung umwandeln.
    +> [Existenzielle Gefahr für alles Leben Zukunft]


<Mind Crime> { Quelle: bostromSuperintelligence }

(1) Eine detaillierte Emulation eines tatsächlichen menschlichen Geistes kann ein Bewusstsein besitzen.
(2) Eine Superintelligenz kann interne Emulationen kreieren, welche einen moralischen Status haben.
(3) Eine Superintelligenz kann Billionen solcher bewusster Emulationen kreieren, um sein Verständnis der menschlichen Psychologie oder Soziologie zu verbessern.
(4) Nachdem die informelle Nützlichkeit dieser Emulationen erschöpft ist, können sie zerstört werden wie Ratten in einem Labor nach dem Versuchsende.
(5) Wenn solche Praktiken auf Simulationen mit hohem moralischen Status angewendet werden, kann das Resultat mit einem Genozid gleichgesetzt werden.
-----
(6) [Mind Crime]: Die Zerstörung einer Emulation mit hohem moralischen Status ist Genozid.
    +> [Ethik leidet]


# Development {isGroup: true}

<Race Dynamics> { Quelle: bostromSuperintelligence }

(1) Ein Wettrennen kommt zu Stande, wenn ein Projekt befürchtet von einem anderen überholt zu werden.
(2) Die Heftigkeit des Wettrennens kommt auf verschiedene Faktoren an.
(3) Faktoren wie Knappheit der Führung, relative Wichtigkeit von Kapazitäten und Glück, Anzahl der Wettbewerber etc. //ist das eine Konkretisierung von 2?
(4) In einem heftigen Wettrennen priorisieren Wettbewerber Geschwindigkeit über Sicherheit.
(5) Ein heftiges Wettrennen kann zu Feindseligkeiten zwischen Wettbewerbern führen.
(6) Das zurückliegende Projekt könnte versucht sein, eine verzweifelten Angriff auf seine Rivalen auszuführen.
(7) Das führende Projekt kann dies antizipieren und einen Präventivangriff ausführen.
(8) Verzweifelte Angriffe und Präventivangriffe haben zerstörende Auswirkungen.
-----
(9) [Devastation]: Die Konsequenzen eines Wettrennens kann zerstörende Auswirkungen haben. //Bitte konkretisieren was "zerstörend" bedeutet.
    +> [Existenzielle Gefahr für alles Leben Zukunft]


<Monitoring> { Quelle: bostromSuperintelligence }

(1) Durch die extremen Sicherheitsimplikationen wird ein mächtiger Staat versuchen, ein Projekt, welches Nahe an der Entwicklung einer Superintelligenz ist, zu verstaatlichen.
(2) Ein mächtiger Staat kann versuchen, Projekte eines anderen Landes, welche Nahe an der Entwicklung einer Superintelligenz sind, durch Spionage, Diebstahl, Kidnapping, Bestechung, Bedrohung, Militäraktionen, etc. zu akquirieren.
(3) Ein mächtiger Staat, dem es nicht gelingt, ein ausländisches Projekt zu akquirieren, kann versuchen, es zu zerstören.
(4) Einmischung in ein ausländisches Projekt kann zu einer internationalen Krise führen.
(5) Zerstörung eines ausländischen Projektes kann zu einer internationalen Krise führen.
-----
(6) [Crisis]: Das Einmischen in oder das Zerstören eines ausländischen Projektes, welches Nahe an der Entwicklung einer Superintelligenz ist, kann zu einer internationalen Krise führen.
    +> [nationale Sicherheit Zukunft]
